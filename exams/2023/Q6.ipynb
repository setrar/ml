{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373e3747-d6c0-4ab6-94cd-56d7c3c9a00b",
   "metadata": {},
   "source": [
    "## 6. Joker card: Linear Regression with Laplacian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6a184-d4b5-44d0-9f86-f6cf588b587c",
   "metadata": {},
   "source": [
    "In the course, we obtain closed-form solution for linear regression (the OLS) by using the maximum likelihood estimation (MLE) under the assumption that our data follows a linear relationship and it has added noise that is normally distributed (Gaussian noise). In this exercice, we will study the usage of different noise model.\n",
    "\n",
    "Let $z \\sim \\text{Laplace}(\\mu, \\beta)$ denote a random valuable that is drawn from an univariate Laplace distribution with mean $\\mu$ and scale parameter $\\beta$. Its PDF is\n",
    "\n",
    "$$\n",
    "p(z,\\mu,\\beta) = \\frac{1}{2\\beta} exp \\Big( \\frac{-|z - \\mu|}{\\beta} \\Big)\n",
    "$$\n",
    "\n",
    "Given $\\mathbf{X}$ the matrix of size $N \\times D$ collecting the inpute data and $y$ the vector of labels of length $N$, our goal is to do linear regression, $y = h(\\mathrm{w}^T \\mathrm{x})$ under the assumption that each label $y_i$ comes from the linear relationship affected by Laplace noise.\n",
    "\n",
    "$$\n",
    "y_i \\sim \\text{Laplace}(\\mathrm{w}^T \\mathrm{x}_i,\\beta)\n",
    "$$\n",
    "\n",
    "You will now use the MLE steps, as seen in the course, to obtain the model parameters $\\mathrm{w}$. For simplicity, we will assume $\\mathrm{w}_0 = 0 $ (no intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f0329-20a6-491c-a909-f57e84b65228",
   "metadata": {},
   "source": [
    "`(a)` [1 point (bonus)] Write the likelihood function $\\mathcal{L}$ for the parameter $\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8589eca-0597-49f9-bb08-2215b10d7e83",
   "metadata": {},
   "source": [
    "To write the likelihood function $\\mathcal{L}$ for the parameter $\\mathbf{w}$, we need to consider the probability of observing the data given the parameters. Given the assumption that each label $y_i$ comes from a Laplace distribution with mean $\\mathbf{w}^T \\mathbf{x}_i$ and scale parameter $\\beta$, the probability density function for each $y_i$ is:\n",
    "\n",
    "$\n",
    "p(y_i \\mid \\mathbf{w}, \\beta) = \\frac{1}{2\\beta} \\exp \\left( \\frac{-|y_i - \\mathbf{w}^T \\mathbf{x}_i|}{\\beta} \\right)\n",
    "$\n",
    "\n",
    "Assuming the data points are independent, the likelihood function $\\mathcal{L}$ for the parameter $\\mathbf{w}$ is the product of the individual probabilities for all data points:\n",
    "\n",
    "$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\prod_{i=1}^N p(y_i \\mid \\mathbf{w}, \\beta)\n",
    "$\n",
    "\n",
    "Substituting the PDF of the Laplace distribution into the likelihood function:\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\prod_{i=1}^N \\frac{1}{2\\beta} \\exp \\left( \\frac{-|y_i - \\mathbf{w}^T \\mathbf{x}_i|}{\\beta} \\right)\n",
    "$\n",
    "\n",
    "This can be further simplified by taking the product inside the exponential term:\n",
    "\n",
    "$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\left(\\frac{1}{2\\beta}\\right)^N \\exp \\left( \\frac{-1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i| \\right)\n",
    "$\n",
    "\n",
    "Thus, the likelihood function for the parameter $\\mathbf{w}$ is:\n",
    "\n",
    "$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\left(\\frac{1}{2\\beta}\\right)^N \\exp \\left( \\frac{-1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i| \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386ec90-1a2f-4f5b-81e5-f8ff19121e66",
   "metadata": {},
   "source": [
    "`(b)` [$1/2$ point (bonus)] Write the log likelihood function $\\log \\mathcal{L}$ for the parameter $\\mathrm{w}$ in the simplest form possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980bb12-7534-4a1e-aa26-124be30369d9",
   "metadata": {},
   "source": [
    "To write the log likelihood function for the parameter $\\mathbf{w}$ in the simplest form possible, we take the natural logarithm of the likelihood function $\\mathcal{L}(\\mathbf{w})$ obtained previously.\n",
    "\n",
    "The likelihood function is:\n",
    "$\n",
    "\\mathcal{L}(\\mathbf{w}) = \\left(\\frac{1}{2\\beta}\\right)^N \\exp \\left( \\frac{-1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i| \\right)\n",
    "$\n",
    "\n",
    "Taking the natural logarithm of both sides:\n",
    "\n",
    "$\n",
    "\\log \\mathcal{L}(\\mathbf{w}) = \\log \\left[ \\left(\\frac{1}{2\\beta}\\right)^N \\exp \\left( \\frac{-1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i| \\right) \\right]\n",
    "$\n",
    "\n",
    "Using the properties of logarithms:\n",
    "\n",
    "$\n",
    "\\log \\mathcal{L}(\\mathbf{w}) = \\log \\left(\\frac{1}{2\\beta}\\right)^N + \\log \\left[ \\exp \\left( \\frac{-1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i| \\right) \\right]\n",
    "$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$\n",
    "\\log \\mathcal{L}(\\mathbf{w}) = N \\log \\left( \\frac{1}{2\\beta} \\right) + \\frac{-1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "$\n",
    "\\log \\mathcal{L}(\\mathbf{w}) = N \\left( \\log 1 - \\log 2 - \\log \\beta \\right) + \\frac{-1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "Since $\\log 1 = 0$:\n",
    "\n",
    "$\n",
    "\\log \\mathcal{L}(\\mathbf{w}) = -N \\log 2 - N \\log \\beta - \\frac{1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "Thus, the log likelihood function for the parameter $\\mathbf{w}$ is:\n",
    "\n",
    "$\n",
    "\\log \\mathcal{L}(\\mathbf{w}) = -N \\log 2 - N \\log \\beta - \\frac{1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "The log likelihood function $\\log \\mathcal{L}$ for the parameter $\\mathbf{w}$ in the simplest form is:\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "\n",
    "$ \\log \\mathcal{L}(\\mathbf{w}) = -N \\log(2\\beta) - \\frac{1}{\\beta} \\sum_{i=1}^{N} |y_i - \\mathbf{w}^T \\mathbf{x}_i|. $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714bed9-f3fb-43e0-abec-b792d7a980d7",
   "metadata": {},
   "source": [
    "`(c)` [1 point (bonus)] What is the simplest loss function that can be used that will lead to the same result as maximizing the likelihood? Write its expression. In which way it is different from that one of the standard least squares regression? ___Hint:___ Question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206aba2-e91b-4197-8557-d4551432e917",
   "metadata": {},
   "source": [
    "Maximizing the likelihood function is equivalent to minimizing the negative log likelihood. From the log likelihood function obtained:\n",
    "\n",
    "$\n",
    "\\log \\mathcal{L}(\\mathbf{w}) = -N \\log 2 - N \\log \\beta - \\frac{1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "We focus on minimizing the term that depends on $\\mathbf{w}$:\n",
    "\n",
    "$\n",
    "-\\frac{1}{\\beta} \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "Since $\\beta$ is a constant, minimizing the negative log likelihood is equivalent to minimizing:\n",
    "\n",
    "$\n",
    "\\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "Therefore, the simplest loss function that can be used is the sum of the absolute errors which is precisely the Least Absolute Deviations (LAD) loss, (also known as the **L1 loss**):\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "$\n",
    "L(\\mathbf{w}) = \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "This is different from the standard least squares regression (OLS), which uses the sum of squared errors (L2 norm) instead of the sum of absolute errors (L1 norm).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023dc90-0d83-4d99-a71a-8673b6188549",
   "metadata": {},
   "source": [
    "`(d)` [2 points (bonus)] Write the batch gradient descent update rule to minimize the cost function from the previous point. \n",
    "___Hint:___ $\\frac{d}{dx}|x| = sign(x)$. You may ignore points where there are undefined gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde78ff5-752f-4c82-a17d-37b07bd326d0",
   "metadata": {},
   "source": [
    "To derive the batch gradient descent update rule to minimize the Least Absolute Deviations (LAD) loss function, we start with the cost function:\n",
    "\n",
    "$\n",
    "L(\\mathbf{w}) = \\sum_{i=1}^N |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "We need to compute the gradient of this cost function with respect to $\\mathbf{w}$. Using the hint that $\\frac{d}{dx}|x| = \\text{sign}(x)$, the gradient with respect to $\\mathbf{w}$ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} = \\sum_{i=1}^N \\frac{\\partial}{\\partial \\mathbf{w}} |y_i - \\mathbf{w}^T \\mathbf{x}_i|\n",
    "$\n",
    "\n",
    "Applying the chain rule and the hint:\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} |y_i - \\mathbf{w}^T \\mathbf{x}_i| = \\text{sign}(y_i - \\mathbf{w}^T \\mathbf{x}_i) \\cdot \\frac{\\partial}{\\partial \\mathbf{w}} (y_i - \\mathbf{w}^T \\mathbf{x}_i)\n",
    "$\n",
    "\n",
    "Since $\\frac{\\partial}{\\partial \\mathbf{w}} (y_i - \\mathbf{w}^T \\mathbf{x}_i) = -\\mathbf{x}_i$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} = \\sum_{i=1}^N \\text{sign}(y_i - \\mathbf{w}^T \\mathbf{x}_i) \\cdot (-\\mathbf{x}_i)\n",
    "$\n",
    "\n",
    "Simplifying this, we get:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} = -\\sum_{i=1}^N \\text{sign}(y_i - \\mathbf{w}^T \\mathbf{x}_i) \\cdot \\mathbf{x}_i\n",
    "$\n",
    "\n",
    "The gradient descent update rule is given by:\n",
    "\n",
    "$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}}\n",
    "$\n",
    "\n",
    "Substituting the gradient we computed:\n",
    "\n",
    "$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\sum_{i=1}^N \\text{sign}(y_i - \\mathbf{w}^T \\mathbf{x}_i) \\cdot \\mathbf{x}_i\n",
    "$\n",
    "\n",
    "Thus, the batch gradient descent update rule to minimize the LAD loss function is:\n",
    "\n",
    "$   \\framebox[1][10]{ Solution: } $\n",
    "\n",
    "\n",
    "$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\sum_{i=1}^N \\text{sign}(y_i - \\mathbf{w}^T \\mathbf{x}_i) \\cdot \\mathbf{x}_i\n",
    "$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b0fd0b-ab04-4a11-bb23-9ebd1d883117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
